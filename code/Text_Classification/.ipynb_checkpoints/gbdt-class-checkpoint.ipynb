{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T06:27:55.925848Z",
     "iopub.status.busy": "2022-07-23T06:27:55.925164Z",
     "iopub.status.idle": "2022-07-23T06:28:07.951469Z",
     "shell.execute_reply": "2022-07-23T06:28:07.950217Z",
     "shell.execute_reply.started": "2022-07-23T06:27:55.925811Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-23T06:27:46.089576Z",
     "iopub.status.busy": "2022-07-23T06:27:46.089107Z",
     "iopub.status.idle": "2022-07-23T06:27:46.133287Z",
     "shell.execute_reply": "2022-07-23T06:27:46.132107Z",
     "shell.execute_reply.started": "2022-07-23T06:27:46.089483Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T06:28:52.634278Z",
     "iopub.status.busy": "2022-07-23T06:28:52.633712Z",
     "iopub.status.idle": "2022-07-23T06:28:52.918646Z",
     "shell.execute_reply": "2022-07-23T06:28:52.917530Z",
     "shell.execute_reply.started": "2022-07-23T06:28:52.634235Z"
    }
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T06:28:20.018978Z",
     "iopub.status.busy": "2022-07-23T06:28:20.018558Z",
     "iopub.status.idle": "2022-07-23T06:28:20.037185Z",
     "shell.execute_reply": "2022-07-23T06:28:20.035436Z",
     "shell.execute_reply.started": "2022-07-23T06:28:20.018943Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to lowercase, strip and remove punctuations\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "\n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "# building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                        or [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T06:33:42.281161Z",
     "iopub.status.busy": "2022-07-23T06:33:42.280188Z",
     "iopub.status.idle": "2022-07-23T06:33:42.610896Z",
     "shell.execute_reply": "2022-07-23T06:33:42.609834Z",
     "shell.execute_reply.started": "2022-07-23T06:33:42.281105Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T06:55:35.587196Z",
     "iopub.status.busy": "2022-07-23T06:55:35.586832Z",
     "iopub.status.idle": "2022-07-23T06:55:35.650934Z",
     "shell.execute_reply": "2022-07-23T06:55:35.650032Z",
     "shell.execute_reply.started": "2022-07-23T06:55:35.587163Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "clf = joblib.load('../input/gbdt-model/GradientBoostingClassifier.pkl')\n",
    "tfidf_vectorizer = joblib.load('../input/gbdt-model/tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T07:07:17.989564Z",
     "iopub.status.busy": "2022-07-23T07:07:17.988899Z",
     "iopub.status.idle": "2022-07-23T07:07:17.996949Z",
     "shell.execute_reply": "2022-07-23T07:07:17.995959Z",
     "shell.execute_reply.started": "2022-07-23T07:07:17.989523Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_privacy(name):\n",
    "    fb = pd.read_csv('../input/order-apps/apps/{}.csv'.format(name),error_bad_lines=False)\n",
    "    fb['clean_text'] = fb['text'].apply(lambda x: finalpreprocess(x))\n",
    "    X_test=fb['clean_text'] \n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "    y_predict = clf.predict(X_vector)\n",
    "    fb['label'] = y_predict\n",
    "    final = fb[['text','label']].reset_index(drop=True)\n",
    "    p = final[final['label']==1]\n",
    "    print(len(p))\n",
    "    with open('{}_privacy.csv'.format(name),'w') as f:\n",
    "        for i in p['text']:\n",
    "            f.write('\"{}\"\\n'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T07:08:58.681034Z",
     "iopub.status.busy": "2022-07-23T07:08:58.680397Z",
     "iopub.status.idle": "2022-07-23T07:08:58.687625Z",
     "shell.execute_reply": "2022-07-23T07:08:58.686549Z",
     "shell.execute_reply.started": "2022-07-23T07:08:58.680997Z"
    }
   },
   "outputs": [],
   "source": [
    "apps = [\"Facebook\",\n",
    "    \"Instagram\",\n",
    "\"Spotify\",\n",
    "\"TikTok\",\n",
    "\"Twitter\",\n",
    "\"YouTube\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-23T07:09:28.338225Z",
     "iopub.status.busy": "2022-07-23T07:09:28.337103Z",
     "iopub.status.idle": "2022-07-23T09:04:16.328481Z",
     "shell.execute_reply": "2022-07-23T09:04:16.327337Z",
     "shell.execute_reply.started": "2022-07-23T07:09:28.338177Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in apps:\n",
    "    find_privacy(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
