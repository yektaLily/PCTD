{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# for model-building\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,recall_score,precision_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase, strip and remove punctuations\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "# building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                        or [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(r'data\\train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel(r'data\\test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train['reviews'].apply(lambda x: finalpreprocess(x))\n",
    "test['clean_text'] = test['reviews'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train['clean_text'][:],test['clean_text'][:],train['label'][:],test['label'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "1659    0\n",
       "1660    0\n",
       "1661    0\n",
       "1662    1\n",
       "1663    1\n",
       "Name: label, Length: 1664, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(X_train)\n",
    "b = list(y_train)\n",
    "c = list(X_test)\n",
    "d = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('train_for_fasttext.txt','w',encoding='utf-8') as f:\n",
    "    for i,j in zip(a,b):\n",
    "        f.write('__label__{}\\t\"{}\"\\n'.format(j,i))\n",
    "with open('test_for_fasttext.txt','w',encoding='utf-8') as f:\n",
    "    for i,j in zip(c,d):\n",
    "        f.write('__label__{}\\t\"{}\"\\n'.format(j,i))\n",
    "with open('train_for_bert.csv','w',encoding='utf-8') as f:\n",
    "    f.write('text,id\\n')\n",
    "    for i,j in zip(a,b):\n",
    "\n",
    "        f.write('\"{}\",{}\\n'.format(i,j))\n",
    "with open('test_for_bert.csv','w',encoding='utf-8') as f:\n",
    "    f.write('text,id\\n')\n",
    "    for i,j in zip(c,d):\n",
    "        \n",
    "        f.write('\"{}\",{}\\n'.format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29c71b42828>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(y_train.value_counts().index,y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok = [nltk.word_tokenize(i) for i in X_train]\n",
    "X_test_tok = [nltk.word_tokenize(i) for i in X_test]\n",
    "# Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# train word2vector model with our training data\n",
    "model = Word2Vec(X_train_tok + X_test_tok, min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8981    0.9327    0.9151       208\n",
      "           1     0.9300    0.8942    0.9118       208\n",
      "\n",
      "    accuracy                         0.9135       416\n",
      "   macro avg     0.9141    0.9135    0.9134       416\n",
      "weighted avg     0.9141    0.9135    0.9134       416\n",
      "\n",
      "pre 0.93\n",
      "recall 0.8942307692307693\n",
      "binary: 0.911764705882353\n"
     ]
    }
   ],
   "source": [
    "# C=10, penalty = 'l2'\n",
    "# FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(C=10)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "r_arr = y_test\n",
    "p_arr = y_predict\n",
    "print(classification_report(r_arr,p_arr,digits=4))\n",
    "print('pre',precision_score(r_arr,p_arr))\n",
    "print('recall',recall_score(r_arr,p_arr))\n",
    "print('binary:', f1_score(r_arr, p_arr, average='binary'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 10}, 0.8880203237875548)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_penalty= LogisticRegression()\n",
    "gsearch1= GridSearchCV(lr_penalty, param_grid={'C':[10,15,20],\n",
    "                                              },cv=5, scoring='f1')\n",
    "gsearch1.fit(X_train_vectors_tfidf, y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8986    0.9375    0.9176       208\n",
      "           1     0.9347    0.8942    0.9140       208\n",
      "\n",
      "    accuracy                         0.9159       416\n",
      "   macro avg     0.9166    0.9159    0.9158       416\n",
      "weighted avg     0.9166    0.9159    0.9158       416\n",
      "\n",
      "binary: 0.914004914004914\n",
      "pre 0.9346733668341709\n",
      "recall 0.8942307692307693\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# C=1,tol=0.0001\n",
    "RFC_tfidf = svm.LinearSVC(C=0.45,tol=0.0001)\n",
    "RFC_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "y_predict = RFC_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = RFC_tfidf.predict(X_test_vectors_tfidf)\n",
    "r_arr = y_test\n",
    "p_arr = y_predict\n",
    "print(classification_report(r_arr,p_arr,digits=4))\n",
    "print('binary:', f1_score(r_arr, p_arr, average='binary'))\n",
    "print('pre',precision_score(r_arr,p_arr))\n",
    "print('recall',recall_score(r_arr,p_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.45, 'tol': 1.2}, 0.8922953572009746)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_penalty= svm.LinearSVC()\n",
    "gsearch1= GridSearchCV(lr_penalty, param_grid={'C':[0.1,1,0.01,0.2,0.3,0.5,1,2,0.45,0.7,0.8],\n",
    "                                                'tol':[1e-4,0.01,1e-3,0.1,1.2,1.8,1e-5,1e-6\n",
    "                                                      ],\n",
    "                                              },cv=5, scoring='f1')\n",
    "gsearch1.fit(X_train_vectors_tfidf, y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8964    0.9567    0.9256       208\n",
      "           1     0.9536    0.8894    0.9204       208\n",
      "\n",
      "    accuracy                         0.9231       416\n",
      "   macro avg     0.9250    0.9231    0.9230       416\n",
      "weighted avg     0.9250    0.9231    0.9230       416\n",
      "\n",
      "binary: 0.9203980099502488\n",
      "pre 0.9536082474226805\n",
      "recall 0.8894230769230769\n"
     ]
    }
   ],
   "source": [
    "# FITTING THE CLASSIFICATION MODEL using RandomForestClassifier(tf-idf)\n",
    "# n_estimators=140,max_depth=11,min_samples_split=50\n",
    "RFC_tfidf = RandomForestClassifier()\n",
    "RFC_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "y_predict = RFC_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = RFC_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "r_arr = y_test\n",
    "p_arr = y_predict\n",
    "\n",
    "print(classification_report(r_arr,p_arr,digits=4))\n",
    "print('binary:', f1_score(r_arr, p_arr, average='binary'))\n",
    "print('pre',precision_score(r_arr,p_arr))\n",
    "print('recall',recall_score(r_arr,p_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 11, 'min_samples_split': 50}, 0.9101515747704519)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':range(3,14,2), 'min_samples_split':range(50,201,25)}\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators= 140),\n",
    "   param_grid = param_test2, scoring='f1', cv=3)\n",
    "gsearch2.fit(X_train_vectors_tfidf, y_train)\n",
    "gsearch2.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 140}, 0.9101515747704519)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {'n_estimators':range(60,151,10)}\n",
    "gsearch1 = GridSearchCV(estimator = RandomForestClassifier(), \n",
    "                       param_grid = param_test1, scoring='f1',cv=5)\n",
    "gsearch1.fit(X_train_vectors_tfidf, y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[203   5]\n",
      " [ 20 188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9103    0.9760    0.9420       208\n",
      "           1     0.9741    0.9038    0.9377       208\n",
      "\n",
      "    accuracy                         0.9399       416\n",
      "   macro avg     0.9422    0.9399    0.9398       416\n",
      "weighted avg     0.9422    0.9399    0.9398       416\n",
      "\n",
      "binary: 0.9376558603491273\n",
      "pre 0.9740932642487047\n",
      "recall 0.9038461538461539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "lr_tfidf = GradientBoostingClassifier(n_estimators=140,random_state=22)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "r_arr = y_test\n",
    "p_arr = y_predict\n",
    "print(confusion_matrix(r_arr,p_arr))\n",
    "print(classification_report(r_arr,p_arr,digits=4))\n",
    "print('binary:', f1_score(r_arr, p_arr, average='binary'))\n",
    "print('pre',precision_score(r_arr,p_arr))\n",
    "print('recall',recall_score(r_arr,p_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer1.pkl']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(lr_tfidf, 'GradientBoostingClassifier1.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer1.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_case.csv','w',encoding='utf-8') as f:\n",
    "    f.write('text,id\\n')\n",
    "    for i,j,z in zip(c,d,p_arr):\n",
    "        \n",
    "        f.write('\"{}\",{},{}\\n'.format(i,j,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 140}, 0.8991274337327009)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'n_estimators':range(135,145,1)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(random_state=22), \n",
    "                       param_grid = param_test2, scoring='f1',cv=3)\n",
    "gsearch1.fit(X_train_vectors_tfidf, y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
